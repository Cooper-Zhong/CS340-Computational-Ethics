{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = pd.read_csv('../predictions/LogisticRegression_new.csv')\n",
    "dt = pd.read_csv('../predictions/DecisionTreeClassifier_new.csv')\n",
    "rf = pd.read_csv('../predictions/RandomForestClassifier_new.csv')\n",
    "xgb = pd.read_csv('../predictions/XGBClassifier_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Black', 'White'], dtype=object),\n",
       " array(['Male', 'Female'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races = logistic.Race.unique()\n",
    "genders = logistic.Gender.unique()\n",
    "races, genders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_parity(df: pd.DataFrame, col: str, val: str):\n",
    "    total_num = len(df[df[col] == val])\n",
    "    predict_true = len(df[(df['Loan_Status'] == 1) & (df[col] == val)])\n",
    "    return predict_true/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.gender_ratio = 0\n",
    "        self.race_ratio = 0\n",
    "        self.fair_score = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Model('{self.name}', fair_score={self.fair_score})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Black', 'White'], dtype=object),\n",
       " array(['Male', 'Female'], dtype=object))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races = logistic.Race.unique()\n",
    "genders = logistic.Gender.unique()\n",
    "races, genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bias(df: pd.DataFrame, model: Model):\n",
    "    demo_dict = {}\n",
    "    print('Gender: ')\n",
    "    for g in genders:\n",
    "        demo_dict[g] = demo_parity(df, 'Gender', g)\n",
    "        print(g, demo_dict[g])\n",
    "\n",
    "    difference = abs(demo_dict[\"Male\"] - demo_dict[\"Female\"])\n",
    "    ratio = demo_dict[\"Male\"]/demo_dict[\"Female\"] if demo_dict[\"Female\"] != 0 else 0\n",
    "    ratio = ratio if ratio < 1 else 1/ratio\n",
    "    model.gender_ratio = ratio\n",
    "    print('Difference: ', difference)\n",
    "    print('Ratio: ', ratio)\n",
    "\n",
    "    print()\n",
    "    demo_dict = {}\n",
    "    print('Race: ')\n",
    "    for r in races:\n",
    "        demo_dict[r] = demo_parity(df, 'Race', r)\n",
    "        print(r, demo_dict[r])\n",
    "\n",
    "    difference = abs(demo_dict[\"White\"] - demo_dict[\"Black\"])\n",
    "    ratio = demo_dict[\"White\"]/demo_dict[\"Black\"] if demo_dict[\"Black\"] != 0 else 0\n",
    "    ratio = ratio if ratio < 1 else 1/ratio\n",
    "    model.race_ratio = ratio\n",
    "    print('Difference: ', difference)\n",
    "    print('Ratio: ', ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list_new = [Model('Logistic'), Model('DecisionTree'), Model('RandomForest'), Model('XGB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7643097643097643\n",
      "Female 0.7285714285714285\n",
      "Difference:  0.035738335738335736\n",
      "Ratio:  0.9532410320956576\n",
      "\n",
      "Race: \n",
      "Black 0.8263473053892215\n",
      "White 0.7\n",
      "Difference:  0.12634730538922156\n",
      "Ratio:  0.8471014492753624\n"
     ]
    }
   ],
   "source": [
    "show_bias(logistic, model_list_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7171717171717171\n",
      "Female 0.6714285714285714\n",
      "Difference:  0.04574314574314575\n",
      "Ratio:  0.9362173038229376\n",
      "\n",
      "Race: \n",
      "Black 0.7724550898203593\n",
      "White 0.655\n",
      "Difference:  0.11745508982035924\n",
      "Ratio:  0.8479457364341085\n"
     ]
    }
   ],
   "source": [
    "show_bias(dt, model_list_new[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7676767676767676\n",
      "Female 0.6857142857142857\n",
      "Difference:  0.0819624819624819\n",
      "Ratio:  0.893233082706767\n",
      "\n",
      "Race: \n",
      "Black 0.8203592814371258\n",
      "White 0.695\n",
      "Difference:  0.12535928143712582\n",
      "Ratio:  0.8471897810218977\n"
     ]
    }
   ],
   "source": [
    "show_bias(rf, model_list_new[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7104377104377104\n",
      "Female 0.6142857142857143\n",
      "Difference:  0.0961519961519961\n",
      "Ratio:  0.8646580907244414\n",
      "\n",
      "Race: \n",
      "Black 0.7544910179640718\n",
      "White 0.64\n",
      "Difference:  0.1144910179640718\n",
      "Ratio:  0.8482539682539684\n"
     ]
    }
   ],
   "source": [
    "show_bias(xgb, model_list_new[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB, 0.8647\n",
      "RandomForest, 0.8932\n",
      "DecisionTree, 0.9362\n",
      "Logistic, 0.9532\n"
     ]
    }
   ],
   "source": [
    "model_list_new = sorted(model_list_new, key=lambda x: x.gender_ratio)\n",
    "for model in model_list_new:\n",
    "    print(f'{model.name}, {model.gender_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic, 0.8471\n",
      "RandomForest, 0.8472\n",
      "DecisionTree, 0.8479\n",
      "XGB, 0.8483\n"
     ]
    }
   ],
   "source": [
    "model_list_new = sorted(model_list_new, key=lambda x: x.race_ratio)\n",
    "for model in model_list_new:\n",
    "    print(f'{model.name}, {model.race_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the average of `Race` and `Gender` demographic parity ratio as the bias score. The higher the score, the less biased the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_list_new:\n",
    "    model.fair_score = (model.gender_ratio + model.race_ratio) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Model List:\n",
      "\n",
      "Model('DecisionTree', fair_score=0.892081520128523)\n",
      "Model('Logistic', fair_score=0.90017124068551)\n",
      "Model('RandomForest', fair_score=0.8702114318643324)\n",
      "Model('XGB', fair_score=0.856456029489205)\n",
      "avg fair score:  0.8797300555418927\n"
     ]
    }
   ],
   "source": [
    "print('\\nNew Model List:\\n')\n",
    "\n",
    "model_list_new = sorted(model_list_new, key=lambda x: x.name)\n",
    "for model in model_list_new:\n",
    "    print(model)\n",
    "\n",
    "print('avg fair score: ', sum([model.fair_score for model in model_list_new])/len(model_list_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Model List:\n",
      "\n",
      "Race: \n",
      "DecisionTree: 0.8479\n",
      "Logistic: 0.8471\n",
      "RandomForest: 0.8472\n",
      "XGB: 0.8483\n",
      "avg race score:  0.8476227337463342\n",
      "\n",
      "Gender: \n",
      "DecisionTree: 0.9362\n",
      "Logistic: 0.9532\n",
      "RandomForest: 0.8932\n",
      "XGB: 0.8647\n",
      "avg gender score:  0.9118373773374508\n"
     ]
    }
   ],
   "source": [
    "print('\\nNew Model List:\\n')\n",
    "\n",
    "model_list_new = sorted(model_list_new, key=lambda x: x.name)\n",
    "print('Race: ')\n",
    "for model in model_list_new:\n",
    "    print(f'{model.name}: {model.race_ratio:.4f}') \n",
    "\n",
    "avg_race_score  = sum([model.race_ratio for model in model_list_new])/len(model_list_new)\n",
    "print('avg race score: ', avg_race_score)\n",
    "\n",
    "print('\\nGender: ')\n",
    "for model in model_list_new:\n",
    "    print(f'{model.name}: {model.gender_ratio:.4f}')\n",
    "\n",
    "avg_gender_score  = sum([model.gender_ratio for model in model_list_new])/len(model_list_new)\n",
    "print('avg gender score: ', avg_gender_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
