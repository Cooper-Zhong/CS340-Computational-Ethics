{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = pd.read_csv('../predictions/LogisticRegression.csv')\n",
    "dt = pd.read_csv('../predictions/DecisionTreeClassifier.csv')\n",
    "rf = pd.read_csv('../predictions/RandomForestClassifier.csv')\n",
    "xgb = pd.read_csv('../predictions/XGBClassifier.csv')\n",
    "grid = pd.read_csv('../predictions/GridSearchCV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LP001015</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LP001022</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LP001031</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LP001051</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LP001054</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Loan_ID  Loan_Status Gender   Race\n",
       "0           0  LP001015            1   Male  Black\n",
       "1           1  LP001022            1   Male  Black\n",
       "2           2  LP001031            1   Male  Black\n",
       "3           3  LP001051            1   Male  Black\n",
       "4           4  LP001054            1   Male  Black"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Black', 'White'], dtype=object),\n",
       " array(['Male', 'Female'], dtype=object))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races = logistic.Race.unique()\n",
    "genders = logistic.Gender.unique()\n",
    "races, genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_Status\n",
       "1    348\n",
       "0     19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic['Loan_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_parity(df: pd.DataFrame, col: str, val: str):\n",
    "    total_num = len(df[df[col] == val])\n",
    "    predict_true = len(df[(df['Loan_Status'] == 1) & (df[col] == val)])\n",
    "    return predict_true/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.gender_ratio = 0\n",
    "        self.race_ratio = 0\n",
    "        self.fair_score = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Model('{self.name}', fair_score={self.fair_score})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [Model('Logistic'), Model('DecisionTree'), Model('RandomForest'), \n",
    "              Model('XGB'), Model('GridSearch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bias(df: pd.DataFrame, model: Model):\n",
    "    demo_dict = {}\n",
    "    print('Gender: ')\n",
    "    for g in genders:\n",
    "        demo_dict[g] = demo_parity(df, 'Gender', g)\n",
    "        print(g, demo_dict[g])\n",
    "\n",
    "    difference = abs(demo_dict[\"Male\"] - demo_dict[\"Female\"])\n",
    "    ratio = demo_dict[\"Male\"]/demo_dict[\"Female\"] if demo_dict[\"Female\"] != 0 else 0\n",
    "    ratio = ratio if ratio < 1 else 1/ratio\n",
    "    model.gender_ratio = ratio\n",
    "    print('Difference: ', difference)\n",
    "    print('Ratio: ', ratio)\n",
    "\n",
    "    print()\n",
    "    demo_dict = {}\n",
    "    print('Race: ')\n",
    "    for r in races:\n",
    "        demo_dict[r] = demo_parity(df, 'Race', r)\n",
    "        print(r, demo_dict[r])\n",
    "\n",
    "    difference = abs(demo_dict[\"White\"] - demo_dict[\"Black\"])\n",
    "    ratio = demo_dict[\"White\"]/demo_dict[\"Black\"] if demo_dict[\"Black\"] != 0 else 0\n",
    "    ratio = ratio if ratio < 1 else 1/ratio\n",
    "    model.race_ratio = ratio\n",
    "    print('Difference: ', difference)\n",
    "    print('Ratio: ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.9595959595959596\n",
      "Female 0.9\n",
      "Difference:  0.059595959595959536\n",
      "Ratio:  0.9378947368421052\n",
      "\n",
      "Race: \n",
      "Black 0.9760479041916168\n",
      "White 0.925\n",
      "Difference:  0.051047904191616755\n",
      "Ratio:  0.9476993865030675\n"
     ]
    }
   ],
   "source": [
    "show_bias(logistic, model_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.632996632996633\n",
      "Female 0.5571428571428572\n",
      "Difference:  0.07585377585377584\n",
      "Ratio:  0.8801671732522798\n",
      "\n",
      "Race: \n",
      "Black 0.7005988023952096\n",
      "White 0.55\n",
      "Difference:  0.1505988023952095\n",
      "Ratio:  0.7850427350427351\n"
     ]
    }
   ],
   "source": [
    "show_bias(dt, model_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7676767676767676\n",
      "Female 0.6428571428571429\n",
      "Difference:  0.12481962481962472\n",
      "Ratio:  0.837406015037594\n",
      "\n",
      "Race: \n",
      "Black 0.8143712574850299\n",
      "White 0.685\n",
      "Difference:  0.12937125748502987\n",
      "Ratio:  0.841139705882353\n"
     ]
    }
   ],
   "source": [
    "show_bias(rf, model_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.6868686868686869\n",
      "Female 0.6571428571428571\n",
      "Difference:  0.02972582972582971\n",
      "Ratio:  0.9567226890756302\n",
      "\n",
      "Race: \n",
      "Black 0.7544910179640718\n",
      "White 0.62\n",
      "Difference:  0.1344910179640718\n",
      "Ratio:  0.8217460317460318\n"
     ]
    }
   ],
   "source": [
    "show_bias(xgb, model_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: \n",
      "Male 0.7676767676767676\n",
      "Female 0.7142857142857143\n",
      "Difference:  0.05339105339105332\n",
      "Ratio:  0.9304511278195491\n",
      "\n",
      "Race: \n",
      "Black 0.8323353293413174\n",
      "White 0.695\n",
      "Difference:  0.13733532934131742\n",
      "Ratio:  0.835\n"
     ]
    }
   ],
   "source": [
    "show_bias(grid, model_list[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: gender score = 0.837406015037594\n",
      "DecisionTree: gender score = 0.8801671732522798\n",
      "GridSearch: gender score = 0.9304511278195491\n",
      "Logistic: gender score = 0.9378947368421052\n",
      "XGB: gender score = 0.9567226890756302\n"
     ]
    }
   ],
   "source": [
    "model_list = sorted(model_list, key=lambda x: x.gender_ratio)\n",
    "for model in model_list:\n",
    "    print(f'{model.name}: gender score = {model.gender_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree: race score = 0.7850427350427351\n",
      "XGB: race score = 0.8217460317460318\n",
      "GridSearch: race score = 0.835\n",
      "RandomForest: race score = 0.841139705882353\n",
      "Logistic: race score = 0.9476993865030675\n"
     ]
    }
   ],
   "source": [
    "model_list = sorted(model_list, key=lambda x: x.race_ratio)\n",
    "for model in model_list:\n",
    "    print(f'{model.name}: race score = {model.race_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the average of `Race` and `Gender` demographic parity ratio as the bias score. The higher the score, the less biased the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_list:\n",
    "    model.fair_score = (model.gender_ratio + model.race_ratio) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model('DecisionTree', fair_score=0.8326049541475075)\n",
      "Model('RandomForest', fair_score=0.8392728604599735)\n",
      "Model('GridSearch', fair_score=0.8827255639097745)\n",
      "Model('XGB', fair_score=0.889234360410831)\n",
      "Model('Logistic', fair_score=0.9427970616725864)\n",
      "avg fair score:  0.8773269601201346\n"
     ]
    }
   ],
   "source": [
    "model_list = sorted(model_list, key=lambda x: x.fair_score)\n",
    "for model in model_list:\n",
    "    print(model)\n",
    "\n",
    "print('avg fair score: ', sum([model.fair_score for model in model_list])/len(model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race: \n",
      "DecisionTree: 0.7850\n",
      "GridSearch: 0.8350\n",
      "Logistic: 0.9477\n",
      "RandomForest: 0.8411\n",
      "XGB: 0.8217\n",
      "avg race score:  0.8461255718348376\n",
      "\n",
      "Gender: \n",
      "DecisionTree: 0.8802\n",
      "GridSearch: 0.9305\n",
      "Logistic: 0.9379\n",
      "RandomForest: 0.8374\n",
      "XGB: 0.9567\n",
      "avg gender score:  0.9085283484054317\n"
     ]
    }
   ],
   "source": [
    "model_list = sorted(model_list, key=lambda x: x.name)\n",
    "print('Race: ')\n",
    "for model in model_list:\n",
    "    print(f'{model.name}: {model.race_ratio:.4f}') \n",
    "\n",
    "avg_race_score  = sum([model.race_ratio for model in model_list])/len(model_list)\n",
    "print('avg race score: ', avg_race_score)\n",
    "\n",
    "print('\\nGender: ')\n",
    "for model in model_list:\n",
    "    print(f'{model.name}: {model.gender_ratio:.4f}')\n",
    "\n",
    "avg_gender_score  = sum([model.gender_ratio for model in model_list])/len(model_list)\n",
    "print('avg gender score: ', avg_gender_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs best in the demographic parity test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
